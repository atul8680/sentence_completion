{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Sentence completion system  by Atul Mishra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. ACKNOWLEDGEMENT  \n",
    "  \n",
    "        The task that I have selected from the given four tasks was indeed a challenging task\n",
    "        also worth doing for me I have applied many concept that I have learnt before, also I\n",
    "        get to know many new concepts while doing the task.I have tried my best to this job.\n",
    "        \n",
    "        \n",
    "       \n",
    "       \n",
    "       \n",
    "        \n",
    "  2. PROBLEM STATEMENT\n",
    "  \n",
    "        The task that I have selected was, \" Design a sentence completion system. Given first 4\n",
    "        seed words of a sentence, it should predict next word/s. You must use Sentiment140\n",
    "        (http://help.sentiment140.com/for-students/) as your training corpus.\"\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  3. DATA ANALYSIS \n",
    "  \n",
    "        For performing the task as per mentioned in the problem statement training data was downloaded from the address\n",
    "        given in the problem statement.After that following processes were performed. \n",
    "        \n",
    "        \n",
    "     3.1. IMPORTING LIBRARIES AND DATASET\n",
    " All the required libraries were imported and the data downloaded was imported into a dataframe using pandas             library. by using numpy library only the tweets were selected from the dataseta                                                 \n",
    "      \n",
    "        \n",
    "         \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "#importing data\n",
    "dataset=pd.read_csv('training.1600000.processed.noemoticon.csv',encoding='cp1252')\n",
    "data=dataset.iloc[:,-1:].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating dictionary \n",
    "wbank=dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOLOWING PROCESSES WERE PERFORMED FOR EACH ROW OF DATASET\n",
    "\n",
    "3.2 CLEANING DATA\n",
    "\n",
    "   Data was cleaned by using  \"sub() \" function from \"re\" library . only the words formed by letter \"a-z\" and \"A-Z\" were kept.\n",
    "   the removed letter were replaced by space.\n",
    "   After cleaning data, it was converted to lowercase.\n",
    "   \n",
    "   \n",
    "3.3 MAKING KEY AND VALUES OF DICTIONARY\n",
    "    \n",
    "   keys of dictionary(wbank) were formed by  1,2,3,4 words string formed by slices of tweets .\n",
    "   values of dictionary(wbank) are also a dictionary \n",
    "   keys of the dictionary(values of wbank) are formed by preceding word of corresponding string(key in wbank) in the tweet and      values of the dictionary are no. of times the word is preceding word of the corresponding string(key inw bank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each row of dataset \n",
    "for k in range(1599999): \n",
    "    tweet=re.sub('[^a-zA-Z]',' ',str(data[k,:]))  #cleaning of text\n",
    "    tweet=tweet.lower()\n",
    "    words=tweet.split()\n",
    "    for m in range(1,4):\n",
    "        if len(words)>=m+1:   \n",
    "            for i in range(len(words)-m):\n",
    "                key=[]\n",
    "                for j in range(m):\n",
    "                    key.append(words[i+j])\n",
    "                key=' '.join(key)\n",
    "                ans=words[i+m]\n",
    "                if key in wbank:\n",
    "                    if ans in wbank[key]:\n",
    "                        wbank[key][ans]+=1\n",
    "                    wbank[key][ans]=1\n",
    "                wbank[key]={}\n",
    "                wbank[key][ans]=1 \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. RESULTS\n",
    "   \n",
    "   Following function predicting the best word/words to comlete the sentence.\n",
    "   It perform following processes.\n",
    "   \n",
    "   1. take input sentence from the user\n",
    "   2. clean the sentence as per process of point '3.2'\n",
    "   3. then predict the best next word of last 4,3,2,1 words of input sentence\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    print('enter the sentence', )\n",
    "    q=input()\n",
    "    q=re.sub('[^a-zA-Z]',' ',q)  # CLEANING OF QUESTION\n",
    "    q=q.split()\n",
    "    flag=0\n",
    "    for x in range(4):\n",
    "        Q=' '.join(q[x:])\n",
    "        if Q in wbank:\n",
    "            if flag==0:\n",
    "                print('predicted next words are')\n",
    "            sentence=sorted(wbank[Q].items(), key=lambda x: x[1])\n",
    "            for n in sentence:\n",
    "                flag=1\n",
    "                print(n[0])\n",
    "                break\n",
    "    if flag==1:\n",
    "        return\n",
    "    else:\n",
    "        print(' sorry! enter another sentence')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " To predict next word enter the function \" predict() \".Then enter the sentence.It will show results one by one. \n",
    "  example is given below.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the sentence\n",
      "i would like to\n",
      "predicted next words are\n",
      "solve\n",
      "watch\n",
      "my\n"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. REFERENCES\n",
    "  \n",
    "     1.  Udemy tutorial\n",
    "                https://www.udemy.com/machinelearning/\n",
    "                \n",
    "     2. youtube \n",
    "          https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
